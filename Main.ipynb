{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb49401-0f33-4ff0-9232-764d281a44c5",
   "metadata": {},
   "source": [
    "# Expanding the Use of Satellite Imagery with Machine Learning  \n",
    "##### Nikolay Zhechev, SoftUni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae23437-9407-4d0c-a1c7-64c03e66b6b5",
   "metadata": {},
   "source": [
    "## Abstract  \n",
    "\n",
    "Satellite imagery provides a unique view of planet Earth and a possibility to see features that were never seeen before. In recent years satellite imagery has become even more essential. Now we are able to see astonishing viwes in detail, narowing to just a few meters.  \n",
    "Collected images have multiple purposes from commmercial and educational to the ability to provide insights on how our Earth is evolving and changing. One of the most fascinating aspects of Earth imagery is that we are able to destinguish meanighful patterns over time helping us with predictions and future outcome. Today humanity has millions of images increasing with around 80TB/day from the past few decades which can help shape our future. Two considerations arise: vast amounts of data and a reproducible method to perfom predictions. Machine Learning can solve both, providing a reproducible and structured way of detecting patterns and important features of sattelite imagery, enabling indepth model selection and tuning for all specific needs. Exporting of sattelite image data and what are good, approachable tecniques and how this data can be fed to a machine learning model to evaluate results, outcomes and determine improvements, reproducibility and further steps.\n",
    "The text aims to further expand the knowledge of the reader and solidify a good understanding of how machine learning concepts can be applyed to sattelite imagery. Both machine learning and specific satellite imagery concepts will be looked at in more detail.\n",
    "\n",
    "## Indroduction  \n",
    "Manual investigation of sattelite imagery can be very time consuming as well as very difficult. Specific software, land and map understanding is required. How can this process be made more efficient? What further value can be generated from a more streamline and effective processing method? The use of machine learning algorithms can solve most of our questions. It should provide a good and stable process of image predictions, pattern recognition and much more.\n",
    "\n",
    "> from: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00772-x  \n",
    "In recent times, Deep Learning, a sophisticated tool in the field of machine learning, has demonstrated its effectiveness in the realm of computer vision and subsequently, in remote sensing as well. The conventional machine learning tools such as Support Vector Machine (SVM) and Random Forest (RF) which are shallow-structured, have major limitations that are addressed by these advanced machine learning algorithms. Prominent deep learning models such as Deep Belief Net (DBN), Stacked Auto-Encoder (SAE), and deep Convolutional Neural Network (CNN) have shown promising results in several remote sensing applications, including segmentation, object detection, and classification. These models are characterized by deep architecture, multi-layered interconnected channels, and a high capacity to learn features.  \n",
    "Nonetheless, the application of these transformers is computationally expensive, and their efficiency decreases exponentially with the size of the image, thereby requiring significant computational resources.\n",
    "\n",
    "Presented in this text are approaches with CNN and SVM. Both methods are used with diiferent datasets to attempt and understand how results differ with data and how these models work and interpret data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0dda76-130e-46eb-8807-9e3b994a93c2",
   "metadata": {},
   "source": [
    "Some use cases include:\n",
    "- agriculture, forestry, and sustainability sectors;\n",
    "- understand local waterway pollution, illegal land uses, or mass migrations;\n",
    "- predict wilwdefire and flood;\n",
    "- predict glacier and water differences;\n",
    "- crops and food predictions;\n",
    "- poverty prediction;\n",
    "- infrastructure and urban planning;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b696c4-0a7e-4690-a801-27353d563e7a",
   "metadata": {},
   "source": [
    "Spatial Resolution: how many meters are covered per pixel, higher resolution will unlock some use cases and give better performance for most algorithms.\n",
    "\n",
    "Instruments available: the instruments will capture different spectral bands, both from the visible and invisible spectrum. Bands usefulness will vary depending on the use case.\n",
    "\n",
    "Temporal resolution : time between two visits on a given spot on earth. Note that many satellite systems actually include several satellites and in that case the global temporal resolution is usually equal to that of one of its satellites divided by the number of satellites.\n",
    "\n",
    "Radiometric resolution : the number of possible values the instrument captures. The higher, the more precise the measurements are.\n",
    "\n",
    "![title](Images/classified_plots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88af872-edf6-4e15-958b-e6763b81db05",
   "metadata": {},
   "source": [
    "Spectral, temporal, and spatial resolution are major features of remote sensing images and are important parameters to be considered during remote sensing image classification process;\n",
    "\n",
    "1.\n",
    "Spectral resolution is composed of different wavelengths of electromagnetic radiation.\n",
    "\n",
    "2.\n",
    "Temporal resolution is the time interval between image acquisitions.\n",
    "\n",
    "3.\n",
    "Spatial resolution is the size of a pixel on the ground. These parameters play a critical role in identifying different land cover types and monitoring changes in land cover over time.\n",
    "\n",
    "They are complex features on remote sensing images and efficient system must be able to effectively process them to achieve accurate classification of remote sensing images by focusing on spectral, temporal, and spatial resolution of the images.\n",
    "\n",
    "There are also other types of remote sensing images based on the nature of the capturing devices. These are categorised into optical, thermal, hyper-spectral, and SAR images:\n",
    "\n",
    "1.\n",
    "Optical images capture visible and near-infrared regions of the electromagnetic spectrum and are the most commonly used remote sensing data for land cover classification.\n",
    "\n",
    "2.\n",
    "Thermal images capture the thermal radiation emitted by the Earth‚Äôs surface and is used to detect temperature variations.\n",
    "\n",
    "3.\n",
    "Hyper-spectral images capture a wide range of spectral bands with narrow bandwidths, allowing for the identification of more subtle spectral signatures.\n",
    "\n",
    "4.\n",
    "SAR images use microwave radiation and can penetrate through clouds and vegetation, making them useful in detecting changes in surface features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce84d8b-4edb-4413-807a-6d099104bd34",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "Let‚Äôs assume you want to train a machine learning model to identify objects in an image it‚Äôs never encountered. The first step in training this supervised machine learning model is to annotate and label a collection of images, called a training dataset. Annotation involves manually identifying and marking the regions of interest in an image.\n",
    "\n",
    "For example, we would annotate each image by outlining what is present in the image and assigning them the corresponding class label. This annotated dataset becomes the foundation for training the model. Once the training dataset is labeled, relevant features need to be extracted from the images. Feature extraction involves identifying and capturing important characteristics or patterns that distinguish one class from another.\n",
    "\n",
    "\n",
    "\n",
    "There are various techniques available for feature extraction in image processing, ranging from simple methods like color histograms and texture descriptors to more advanced approaches like convolutional neural networks (CNNs).\n",
    "\n",
    "The training process of a supervised ML model, like a CNN, involves several steps. First, the data needs to be preprocessed to ensure consistency and quality. This may involve resizing the images, normalizing pixel values, and augmenting the dataset by applying transformations like rotations or flips to increase its diversity.\n",
    "\n",
    "\n",
    "\n",
    "CNNs‚Äô architecture tries to mimic the structure of neurons in the human visual system composed of multiple layers, where each one is responsible for detecting a specific feature in the data.  As illustrated in the image below, the typical CNN is made of a combination of four main layers: \n",
    "\n",
    "- Convolutional layers  \n",
    "- Rectified Linear Unit (ReLU for short)  \n",
    "- Pooling layers  \n",
    "- Fully connected layersÔøΩùëíùëë . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102023a-a0bd-4479-a9d9-7013e0008a1a",
   "metadata": {},
   "source": [
    "Spectral indices, which are features computed from two or\n",
    "more spectral bands, are commonly used in place of or to\n",
    "supplement the bands. The most commonly used index is the\n",
    "normalized difference vegetation index (NDVI), which\n",
    "compares the values of the red and near-infrared (NIR) band\n",
    "using this formula:  \n",
    "\n",
    "$ NDVI = \\frac{(NIR‚àíRED)}{(NIR+RED)} $\n",
    "\n",
    "NDVI quantifies vegetation by measuring the difference between near-infrared (which vegetation strongly reflects) and red light (which vegetation absorbs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af04d37-ccf5-4fe0-b2e1-d2b20d16b701",
   "metadata": {},
   "source": [
    "Unsupervised learning does not rely on labeled data but instead aims to discover hidden patterns, structures, or relationships within the data itself.\n",
    "\n",
    "The purpose of unsupervised learning in image analysis is to uncover meaningful structures and insights from unlabeled image data. By utilizing unsupervised learning techniques, valuable information can be extracted and a deeper understanding is gained of the underlying characteristics of images.\n",
    "\n",
    "Unsupervised learning can help identify clusters of similar images, discover patterns or textures that are characteristic of certain image classes, and detect anomalies or outliers within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a97659-486c-4a9a-95b4-f887f352323b",
   "metadata": {},
   "source": [
    "### Supervised Classification\n",
    "#### Land Cover Mapping \n",
    "For implemention see section 1.1, sub section 1.1.1  \n",
    "\n",
    "Categorize areas into land cover types like urban, forest, or water. Enables monitoring of land cover changes over time with land cover classification using CART (Classification and Regression Tree) in Earth Engine.  \n",
    "Data:  \n",
    "- Collect images from Landsat 8 (LANDSAT/LC08/C02/T1_L2).\n",
    "- Scale and mask each image.\n",
    "- Creates a cloud-free composite by taking the median value for each pixel across all images\n",
    "\n",
    "\n",
    "Preprocess:\n",
    "- Remove unwanted pixels.\n",
    "- Select visible, near-infrared (NIR), shortwave-infrared (SWIR), and thermal bands as input features for the classification model.\n",
    "- Scale and offset.\n",
    "- Apply mask.\n",
    "\n",
    "\n",
    "Training:  \n",
    "- Uses a predefined FeatureCollection (demo_landcover_labels) containing points with known land cover classes.  \n",
    "- Each point has a landcover property that stores numeric labels (e.g., 0 = Urban, 1 = Forest, 2 = Water).\n",
    "\n",
    "\n",
    "Classifier:  \n",
    "- Uses a CART (Classification and Regression Tree) model. CART splits data based on decision rules to classify pixels into categories.  \n",
    "\n",
    "\n",
    "Applies the trained classifier to the entire image, assigning each pixel a land cover class (e.g., 0 = Urban, 1 = Forest, 2 = Water).\n",
    "\n",
    "Visualization:  \n",
    "- Setp map coordinates to predefined ones.\n",
    "- Add input image visualizing the cloud-free compoiste with specific bands (RGB bands).\n",
    "- Displays the classified image with land cover classes:\n",
    "    - Orange: Urban (class 0)\n",
    "    - Green: Forest (class 1)\n",
    "    - Blue: Water (class 2)\n",
    " \n",
    "<img src=\"Images/ee_pred_1.png\" width=\"600\" />\n",
    "fig. 01\n",
    "\n",
    "\n",
    "\n",
    "#### Identify areas of forest loss  \n",
    "For implemention see section 1.1, sub section 1.1.2  \n",
    "\n",
    "\n",
    "Processes Landsat 8 surface reflectance data to create a cloud-free composite, prepare training data, train a classifier, and then classify the image to identify areas of deforestation. Represent forested and non-forested areas.  \n",
    "\n",
    "FeatureCollection:\n",
    "- Combines polygons into a dataset with a class property (1 for forest, 0 for non-forest).  \n",
    "Classifier:  \n",
    "- SVM classifier with a radial basis function (RBF) kernel.\n",
    "\n",
    "  \n",
    "Apply the trained classifier to the entire composite image, labeling each pixel as either forest (1) or non-forest (0).  \n",
    "Displays the composite with bands SR_B4 (red), SR_B3 (green), and SR_B2 (blue), which are in the visible spectrum.\n",
    "\n",
    "Classification Result:  \n",
    "- Displays the classified map with colors:  \n",
    "    - Orange: Non-forest (class 0)  \n",
    "    - Green: Forest (class 1)\n",
    " \n",
    "\n",
    "<img src=\"Images/ee_pred_2.png\" width=\"600\" />\n",
    "fig. 02\n",
    "\n",
    "\n",
    "#### Land Cover Analysis  \n",
    "For implemention see section 1.1, sub section 1.1.3  \n",
    "\n",
    "\n",
    "Identify areas of forest, water, urban, etc. Detects land cover changes over time for environmental monitoring. a Using Random Forest (RF) classifier to train and validate a land cover classification model. \n",
    "\n",
    "- Define latitude and longitude.\n",
    "- Retreive Landsat 8 Tier 1 Level 2 data for the defined ROI (region of interest).\n",
    "- Load MODIS International Geosphere-Biosphere Programme (IGBP) land cover classification as labels for training (contains 17 land cover classes (e.g., Forest, Urban, Water).\n",
    "- Select speicif bands SR_B2 (Blue) to SR_B7 (SWIR2) for anlysis.\n",
    "- Take the median value for each pixel (median composite) to create cloud-free images.\n",
    "\n",
    "Combine the input image bands and MODIS labels into a single dataset. Randomly samples 5,000 points across the ROI to create a FeatureCollection of training data. Where each point has:  \n",
    "- Landsat band values (SR_B2 to SR_B7) as features.\n",
    "- MODIS land cover label (LC_Type1) as the target.\n",
    "- Split the data into training (70%) and validation (30%) sets.\n",
    "\n",
    "Random Forest Classifier:  \n",
    "- Algorithm: Constructs an ensemble of decision trees.\n",
    "- Features: Reflectance values from bands SR_B2 to SR_B7.\n",
    "- Target (classProperty): MODIS land cover label (LC_Type1).\n",
    "- We use 10 tress in the forest for higher accuracy.\n",
    "- Outputs a classified map where each pixel is assigned a land cover class (0‚Äì16).\n",
    "\n",
    "Model evaluation:\n",
    "- Confusion Matrix for training accuracy: Compares predicted vs. true labels.\n",
    "- Validation Accuracy for validation accuracy: Summarizes true positives, false positives, etc.\n",
    "\n",
    "Outputs a classified land cover map.\n",
    "\n",
    "Training size: 3515\n",
    "Training overall accuracy: 0.9513888888888888\n",
    "Validation overall accuracy: 0.6299259851970395\n",
    "\n",
    "<img src=\"Images/ee_pred_3.png\" width=\"600\" />\n",
    "\n",
    "fig. 03\n",
    "\n",
    "\n",
    "<img src=\"Images/matrixs_pred.png\" width=\"600\" />\n",
    "\n",
    "fig. 04\n",
    "\n",
    "\n",
    "\n",
    "#### SVM Image Classifier with TensorFlow  \n",
    "For implemention see section 1.1, sub section 1.1.4  \n",
    "\n",
    "|               | precision      | recall     | f1-score      | support      |\n",
    "| ------------- | ------------- | ------------- | ------------- | ------------- |\n",
    "| cloudy | 1.00 | 1.00 | 1.00 | 268 |\n",
    "| desert | 1.00 | 1.00 | 1.00 | 256 |\n",
    "| green_area |  0.88 | 0.93 | 0.91 |305 |\n",
    "| water | 0.92 | 0.88 | 0.90 | 297 |\n",
    "| accuracy |   |   | 0.95 | 1126 |\n",
    "| macro avg | 0.95  | 0.95 | 0.95 | 1126 |\n",
    "| weighted avg | 0.95  | 0.95 | 0.95 | 1126 |\n",
    "\n",
    "<img src=\"Images/classification_images.png\" width=\"600\" />\n",
    "\n",
    "fig. 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bb5f2-d688-4657-9ba4-7225ce2b2f5b",
   "metadata": {},
   "source": [
    "#### Pattern Identification\n",
    "For implemention see section 1.2, sub section 1.2.1  \n",
    "\n",
    "Clustering does not rely on labeled training data. Instead, it groups data points based on their inherent characteristics (spectral similarity). using K-means Clustering where the algorithm divides the data into k clusters by minimizing the variance within each cluster and maximizing the variance between clusters. Useful for identifying patterns or features such as vegetation zones, water bodies, or urban areas in remote sensing images without prior labels.  \n",
    "\n",
    "Each pixel contains spectral information from the bands in the image.\n",
    "K-means Clustering:\n",
    "- Here we specify k=15 clusters.\n",
    "- The algorithm minimizes intra-cluster variance, ensuring pixels in the same cluster are spectrally similar.\n",
    "- The trained clusterer is applied to the entire image to assign each pixel to one of the 15 clusters.\n",
    "- The output is a single-band image where each pixel‚Äôs value corresponds to its cluster ID (an integer between 0 and 14).\n",
    "  \n",
    "Training:\n",
    "- The clusterer is trained using the sampled pixel data, allowing it to learn the spectral characteristics of the region.\n",
    "\n",
    "The map shows 15 distinct clusters representing different spectral characteristics of the landscape. Result is not tied to specific land cover types but provides insight into natural groupings in the data.\n",
    "\n",
    "<img src=\"Images/ee_pred_4.png\" width=\"600\" />\n",
    "\n",
    "fig. 06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47e9f4-7e3d-43c6-b5be-81ba2a1fdd85",
   "metadata": {},
   "source": [
    "### Supervised Classification\n",
    "For implemention see section 2\n",
    "\n",
    "#### Data preprocess and masking\n",
    "section 2.1\n",
    "\n",
    "Building: #3C1098  \n",
    "Land (unpaved area): #8429F6  \n",
    "Road: #6EC1E4  \n",
    "Vegetation: #FEDD3A  \n",
    "Water: #E2A929  \n",
    "Unlabeled: #9B9B9B  \n",
    "\n",
    "| Satellite Image             |  Mask |\n",
    "|-----------------------------|-------------------------|\n",
    "| <img src=\"Datasets/Segmentation_Tasks/Semantic%20segmentation%20dataset/Tile%201/images/image_part_009.jpg\" width=\"300\" /> | <img src=\"Datasets/Segmentation_Tasks/Semantic%20segmentation%20dataset/Tile%201/masks/image_part_009.png\" width=\"300\" /> |\n",
    "| <img src=\"Datasets/Segmentation_Tasks/Semantic%20segmentation%20dataset/Tile%205/images/image_part_007.jpg\" width=\"300\" /> | <img src=\"Datasets/Segmentation_Tasks/Semantic%20segmentation%20dataset/Tile%205/masks/image_part_007.png\" width=\"300\" /> |\n",
    "\n",
    "\n",
    "\n",
    "Break large images into smaller patches and normalize them, to ensure the dataset is ready for patch-based processing and semantic segmentation.\n",
    "\n",
    "For image patches:\n",
    "\n",
    "Min-max normalization is applied using minmaxscaler.\n",
    "Each normalized patch is stored in image_dataset.\n",
    "For mask patches:\n",
    "\n",
    "Patches are directly stored in mask_dataset.\n",
    "\n",
    "#### One-hot encoding\n",
    "section 2.2\n",
    "\n",
    "- using classes.json  \n",
    "\n",
    "Each class e.g., building, land, road is represented by a specific color. During one-hot encoding of mask images, these RGB values can be used to identify pixels corresponding to each class and map them to class indices or binary masks.  \n",
    "\n",
    "Extract the red ('3C'), green ('10'), and blue ('98') values and converts them to integers using int(hex, 16).  \n",
    "The output for each class is an array of three integers, each representing the RGB values.\n",
    "\n",
    "[ 60  16 152]  \n",
    "[132  41 246]  \n",
    "[110 193 228]  \n",
    "[254 221  58]  \n",
    "[226 169  41]  \n",
    "[155 155 155]  \n",
    "\n",
    "Models expect masks in the form of class indices (e.g., 0 for water, 1 for land).\n",
    "\n",
    "For example:  \n",
    "[226, 169, 41] corresponds to class 0 (water),  \n",
    "[155, 155, 155] corresponds to class 5 (unlabeled),  \n",
    "[254, 221, 58] corresponds to class 4 (vegetation),  \n",
    "[110, 193, 228] corresponds to class 2 (road)  \n",
    "\n",
    "\n",
    "#### Architecture\n",
    "section 2.3\n",
    "\n",
    "\n",
    "The Jaccard coefficient is defined as:\n",
    "\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "- $|A \\cap B| $: The intersection, already computed.\n",
    "- $ |A \\cup B| $: The union, calculated as:\n",
    "\n",
    "  $$|A \\cup B| = |A| + |B| - |A \\cap B|$$\n",
    "\n",
    "\n",
    "Adding \\( 1.0 \\) to the numerator and denominator is a smoothing term to avoid division by zero when both `y_true` and `y_pred` are empty (i.e., when the intersection and union are zero).\n",
    "\n",
    "\n",
    "Follow U-Net architecture > https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/  \n",
    "\n",
    "![u-net-arch-img-link](Images/u-net-architecture.png)\n",
    "\n",
    "\n",
    "-  convulution parameter (first number) is changed from original architecture due to our use case (for every next layer filter needs to be doubled)\n",
    "-  dropout parameter can be changed and experimented for use case\n",
    "\n",
    "#### Define Loss Function \n",
    "section 2.4\n",
    "\n",
    "dice loss > Focal Loss > Total Loss  \n",
    "Total Loss = (Dice loss + (1*Focal Loss))  \n",
    "\n",
    "\n",
    "Class weights are used to balance the loss function in cases where some classes are more prevalent than others in the dataset (i.e., imbalanced datasets).  \n",
    "Dice loss measures the overlap between predicted segmentation and ground truth. `class_weights` will adjust the contribution of each class.     \n",
    "Dice loss: Focuses on overlap quality, improving segmentation performance.  \n",
    "Focal loss: Emphasizes learning on hard-to-classify examples, addressing class imbalance.\n",
    "\n",
    "\n",
    "#### Model compliation\n",
    "section 2.5\n",
    "\n",
    "Model: \"functional\"\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "section 2.6\n",
    "\n",
    "\n",
    "|   |   |\n",
    "|-----------------------------|-------------------------|\n",
    "<img src=\"Images/loss_1_segm.png\" width=\"350\" /> | <img src=\"Images/loss_intersect_segm.png\" width=\"350\" />\n",
    "\n",
    "\n",
    "\n",
    "#### Generate predictions\n",
    "section 2.7\n",
    "\n",
    "Compare test mask and predicted mask images  \n",
    "\n",
    "<img src=\"Images/segm_pred_1.png\" width=\"650\" />\n",
    "<img src=\"Images/segm_pred_2.png\" width=\"650\" />\n",
    "<img src=\"Images/segm_pred_3.png\" width=\"650\" />\n",
    "<img src=\"Images/segm_pred_4.png\" width=\"650\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18222b2-b87e-47a5-8fd8-9822f4d35e9e",
   "metadata": {},
   "source": [
    "## *Conclusion - outcome>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464e359-a0d8-4156-845a-21b5957236a9",
   "metadata": {},
   "source": [
    "## *Cite and resources >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
