{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8845c7b0-d067-4271-81be-1106ebc9b421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Earth Engine Python client library\n",
    "import ee\n",
    "import geemap.core as geemap\n",
    "\n",
    "# Resterio to load files\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv\n",
    "import cv2  # OpenCV for faster resizing\n",
    "import glob\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "# TorchGeeo\n",
    "# https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/\n",
    "from lightning.pytorch import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchgeo.datamodules import InriaAerialImageLabelingDataModule\n",
    "from torchgeo.datasets import CDL, Landsat7, Landsat8, VHR10, stack_samples\n",
    "from torchgeo.samplers import RandomGeoSampler\n",
    "from torchgeo.trainers import SemanticSegmentationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b860fb36-6eda-4155-8965-b1b14fb6f584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Earth Engine Python client library config\n",
    "# Authenticate (redirect to auth page if not)\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80e3165c-03ee-4711-89d0-7f0fc0fb0584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Google project\n",
    "ee.Initialize(project = 'ee-nikolaydragomirovzhechev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "88462339-1384-4e63-8999-5d61ce619585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "debug_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59d66e-3c2e-40f9-84ee-de4573c54171",
   "metadata": {},
   "source": [
    "Locate agricultural areas on large satellite images\n",
    "\n",
    "Detect and outline the border of each plot within those areas\n",
    "\n",
    "Classify the crops of these plots (wheat, tomatoes, corn …)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4da61-7016-430e-92d2-8555983f80b2",
   "metadata": {},
   "source": [
    "Scalability: A bank of images covering the whole world is available right away and being updated regularly\n",
    "\n",
    "Data richness: Satellite images can provide a lot more information than simple pictures. Instead of a 3-band image of Red, Green and Blue pixels, some satellites can provide more than 15 features per pixel\n",
    "\n",
    "Cost: Even though satellite imagery can be quite costly, some options are fully free, such as Sentinel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e9b1bdbc-dfc1-472f-8f71-1984417942e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Landsat surface reflectance data\n",
    "# Continuous Change Detection and Classification (CCDC) algorithm\n",
    "dataset_Landsat = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f146091-38c6-46dc-9543-8773ad9094be",
   "metadata": {},
   "source": [
    "longitude_min: The minimum longitude (western boundary) of your AOI.  \n",
    "latitude_min: The minimum latitude (southern boundary) of your AOI.  \n",
    "longitude_max: The maximum longitude (eastern boundary) of your AOI.  \n",
    "latitude_max: The maximum latitude (northern boundary) of your AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24772cfe-a7d6-40d5-b917-bdd7d5772e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def define_area_of_interest(coordinates_urban, coordinates_forest):\n",
    "    \"\"\"Defines and returns areas of interests. Takes coordinates as parameters.\"\"\"\n",
    "    \n",
    "    aoi_urban = ee.Geometry.Rectangle(coordinates_urban)  # Example: urban area\n",
    "    aoi_forest = ee.Geometry.Rectangle(coordinates_forest)  # Example: forest area\n",
    "\n",
    "    return aoi_forest, aoi_urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b307642-a6d3-4a37-bb8a-bbd265d9e9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839\n"
     ]
    }
   ],
   "source": [
    "dataset_filtered = dataset_Landsat.filterDate(start_date, end_date).filterBounds(aoi_forest)\n",
    "print(dataset_filtered.size().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e022e-6fd4-416d-8162-74c8485ee786",
   "metadata": {},
   "source": [
    "One of the most commonly used ones in an agricultural context is the NDVI (Normalized Difference Vegetation Index). This index is used to estimate the density of vegetation on the ground, which could serve to detect agricultural areas over a large image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a525905-374c-4541-bc89-afda5cd13ce3",
   "metadata": {},
   "source": [
    "NDVI info:  \n",
    "https://gisgeography.com/ndvi-normalized-difference-vegetation-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d990bcb6-5af0-4c22-826c-c91202c675d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_ndvi(image):\n",
    "    ndvi = image.normalizedDifference(['B4', 'B5']).rename('NDVI')  # ['SR_B7', 'SR_B4']\n",
    "    return image.addBands(ndvi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab56b33-620e-43ba-a2fd-d510f9f8a348",
   "metadata": {},
   "source": [
    "Bands provide information about different spectral characteristics derived from Landsat satellite imagery.  \n",
    "Key Bands in the Dataset:\n",
    "Blue (B1):\n",
    "\n",
    "Wavelength: ~450-520 nm\n",
    "Useful for: Coastal and aerosol studies, distinguishing soil from vegetation, and monitoring water bodies.\n",
    "Green (B2):\n",
    "\n",
    "Wavelength: ~520-600 nm\n",
    "Useful for: Assessing vegetation vigor, distinguishing between different types of vegetation, and monitoring aquatic systems.\n",
    "Red (B3):\n",
    "\n",
    "Wavelength: ~630-690 nm\n",
    "Useful for: Discriminating between vegetation types, monitoring crop health, and assessing urban vs. rural areas.\n",
    "Near-Infrared (NIR, B4):\n",
    "\n",
    "Wavelength: ~770-900 nm\n",
    "Useful for: Vegetation health, crop monitoring, and distinguishing between water bodies and land.\n",
    "Shortwave Infrared 1 (SWIR1, B5):\n",
    "\n",
    "Wavelength: ~1550-1750 nm\n",
    "Useful for: Assessing vegetation moisture, soil moisture, and differentiating between snow, clouds, and other features.\n",
    "Shortwave Infrared 2 (SWIR2, B6):\n",
    "\n",
    "Wavelength: ~2080-2350 nm\n",
    "Useful for: Monitoring droughts, soil moisture, and differentiating between snow/ice and clouds.\n",
    "Thermal Infrared (TIR, B10):\n",
    "\n",
    "Wavelength: ~10.4-12.5 µm\n",
    "Useful for: Monitoring temperature, surface heat emissions, and identifying hot spots (e.g., wildfires).\n",
    "Special Bands from the CCDC Algorithm:\n",
    "Trend Coefficients (Slope and Intercept):\n",
    "\n",
    "These bands represent the trend of surface reflectance values over time, generated by the CCDC algorithm. The slope indicates whether the reflectance has increased or decreased, while the intercept gives the baseline value at the beginning of the time series.\n",
    "Seasonality Components:\n",
    "\n",
    "CCDC decomposes time-series data into seasonal components, capturing cyclic patterns such as seasonal vegetation growth.\n",
    "Breakpoint Count:\n",
    "\n",
    "This band records the number of \"breakpoints\" where land cover change was detected by the CCDC algorithm over the 1999-2019 period.\n",
    "Segment Number:\n",
    "\n",
    "Each pixel in the CCDC dataset is divided into segments based on change over time. The segment number indicates which part of the time series the pixel belongs to (e.g., a stable segment or a transition due to land cover change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bc9e291-0f9b-496e-b696-eb0fdc3b6735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3db245c34f743f0b1fbc1f76346d95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[35.619108968819454, -109.85], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_with_ndvi = dataset_filtered.map(compute_ndvi)\n",
    "\n",
    "# Get a median composite of the NDVI band\n",
    "ndvi_composite = dataset_with_ndvi.select('NDVI').median()\n",
    "\n",
    "# Visualize the result\n",
    "m = geemap.Map()\n",
    "m.centerObject(aoi_urban, 10)\n",
    "m.addLayer(ndvi_composite, {'min': 0, 'max': 1, 'palette': ['white', 'green', 'brown']}, 'NDVI')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821343b-2459-4691-bd1a-f0734e95908f",
   "metadata": {},
   "source": [
    "#### Export Data to a Machine Learning-Compatible Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6ffd4-ca4d-4334-aa11-3b9c534bebbb",
   "metadata": {},
   "source": [
    "This code performs an export task. Exported tasks can be saved in a personal Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61cb7961-a74b-4b33-abb5-53bac8aeda6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def export (data, region, description):\n",
    "    '''Initiate Google Cloud export task. Provide data and region.'''\n",
    "    # export task\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image = data,\n",
    "        description = description,\n",
    "        scale = 30, # The resolution of the image in meters\n",
    "        region = region,\n",
    "        fileFormat = 'GeoTIFF',\n",
    "        maxPixels = 9485956438\n",
    "    )\n",
    "    \n",
    "    task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c1b05-31ef-4ee1-8686-fdc955611bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "export(ndvi_composite, aoi_urban)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4ce41-104e-46cf-ac48-9bee51c887ce",
   "metadata": {},
   "source": [
    "Load exported results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bdc489b-578c-4f9f-a94d-f2ea5fc4313b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image metadata: {'driver': 'GTiff', 'dtype': 'float32', 'nodata': None, 'width': 2598, 'height': 1858, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(0.00026949458523585647, 0.0, -119.7000624512842,\n",
      "       0.0, -0.00026949458523585647, 35.00060925750686)}\n",
      "Number of bands: 1\n"
     ]
    }
   ],
   "source": [
    "with rasterio.open('Datasets/Landsat/Landsat_NDVI_Export_01-0000000000-0000000000') as src:\n",
    "    print(\"Image metadata:\", src.meta)\n",
    "\n",
    "    # Get number of bands\n",
    "    num_bands = src.count\n",
    "    print(f\"Number of bands: {num_bands}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18b08539-e916-4067-b9be-a0161a09b4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Open the exported GeoTIFF\n",
    "def load_geotiff_img(img_path, new_img_name):\n",
    "    '''Loads and open downloaded GeoTIFF images.'''\n",
    "    with rasterio.open('Datasets/{}'.format(img_path)) as src:\n",
    "        # Read all bands into a NumPy array\n",
    "        image_data = src.read()  # Shape: (bands, height, width)\n",
    "    \n",
    "        # Select the NDVI band (assuming it's stored in one of the bands)\n",
    "        ndvi_band = src.read(1)  # Replace '1' with the correct band number if necessary\n",
    "    \n",
    "        # Normalize NDVI band between 0 and 1\n",
    "        ndvi_normalized = (ndvi_band - ndvi_band.min()) / (ndvi_band.max() - ndvi_band.min())\n",
    "    \n",
    "        # If needed, convert the image data from a 3D to 4D format (e.g., for CNN input)\n",
    "        image_data = np.transpose(image_data, (1, 2, 0))  # Shape: (height, width, bands)\n",
    "    \n",
    "        # Expand dimensions to add the channel, then resize using skimage's resize function\n",
    "        X = np.expand_dims(ndvi_normalized, axis=-1)  # Shape: (height, width, 1)\n",
    "        Resize the image to 256x256 (for CNN input) \n",
    "        X_resized = resize(X, (256, 256, 1), mode='reflect', anti_aliasing=True)\n",
    "        X_resized = X_resized.astype(np.float32)\n",
    "        print(f\"Shape of resized X: {X_resized.shape}\")  # Should be (256, 256, 1)\n",
    "    \n",
    "        # Get the image size (width, height)\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "        num_channels = src.count  # Number of bands\n",
    "    \n",
    "        # Define the new dimensions (e.g., 256x256 pixels)\n",
    "        new_width, new_height = 256, 256\n",
    "        scale_factor_x = new_width / width\n",
    "        scale_factor_y = new_height / height\n",
    "    \n",
    "        # Resize the image using the 'average' resampling method\n",
    "        resized_img = src.read(\n",
    "            out_shape=(src.count, new_height, new_width),  # (num_bands, height, width)\n",
    "            resampling=Resampling.average\n",
    "        )\n",
    "    \n",
    "        # Save the resized image\n",
    "        with rasterio.open(new_img_name, 'w', \n",
    "                           driver=src.driver, \n",
    "                           width=new_width, \n",
    "                           height=new_height, \n",
    "                           # count=src.count, \n",
    "                           count=resized_img.shape[-1],\n",
    "                           dtype=src.dtypes[0], \n",
    "                           crs=src.crs, \n",
    "                           transform=src.transform) as dst:\n",
    "            dst.write(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "271b2774-31d3-49b9-8361-6e856064f56f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_geotiff_img_advanced(img_path, new_img_name, selected_bands=None):\n",
    "    \"\"\"\n",
    "    Loads, normalizes, resizes, and saves GeoTIFF images for model training.\n",
    "    Works for single-band (e.g., NDVI) or multi-band images. \n",
    "    Ensures output shape is (256, 256, bands).\n",
    "    \"\"\"\n",
    "    with rasterio.open(f'Datasets/{img_path}') as src:\n",
    "        # Determine the bands to process\n",
    "        num_bands = src.count\n",
    "        bands_to_process = selected_bands if selected_bands else range(1, num_bands + 1)\n",
    "        nodata_value = src.nodata\n",
    "\n",
    "        # Initialize list to store resized bands\n",
    "        resized_bands = []\n",
    "\n",
    "        for band_idx in bands_to_process:\n",
    "            # Read the band\n",
    "            band_data = src.read(band_idx)\n",
    "\n",
    "            # Handle no-data values\n",
    "            if nodata_value is not None:\n",
    "                band_data = np.where(band_data == nodata_value, np.nan, band_data)\n",
    "\n",
    "            # Normalize the band\n",
    "            band_clipped = np.clip(band_data, -1, 1) if selected_bands == ['NDVI'] else band_data\n",
    "            band_normalized = (band_clipped + 1) / 2 if selected_bands == ['NDVI'] else band_data\n",
    "\n",
    "            # Resize the band to 256x256\n",
    "            band_resized = resize(band_normalized, (256, 256), mode='reflect', anti_aliasing=True).astype(np.float32)\n",
    "            resized_bands.append(band_resized)\n",
    "\n",
    "        # Stack bands for multi-band output\n",
    "        resized_image = np.stack(resized_bands, axis=-1)  # Shape: (256, 256, bands)\n",
    "        \n",
    "        # Adjust transform for resizing\n",
    "        transform_resized = src.transform * src.transform.scale(\n",
    "            src.width / 256, src.height / 256\n",
    "        )\n",
    "\n",
    "        # Save the resized image\n",
    "        with rasterio.open(\n",
    "            new_img_name,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=256,\n",
    "            width=256,\n",
    "            count=resized_image.shape[-1],  # Number of bands\n",
    "            dtype='float32',\n",
    "            crs=src.crs,\n",
    "            transform=transform_resized,\n",
    "        ) as dst:\n",
    "            for i in range(resized_image.shape[-1]):  # Loop over bands\n",
    "                dst.write(resized_image[:, :, i], i + 1)  # Write each band\n",
    "\n",
    "        print(f\"Saved resized image with {resized_image.shape[-1]} band(s) to {new_img_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3e08965-b59d-4133-ba6d-ce3f4a0a2920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_geotiff_img_optimized(img_path, selected_bands, output_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Optimized function to load and preprocess GeoTIFF images.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the GeoTIFF image.\n",
    "        selected_bands (list): List of band indices or names to process.\n",
    "        output_size (tuple): Target size for resizing (width, height).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Resized and processed image with shape (height, width, channels).\n",
    "    \"\"\"\n",
    "    resized_bands = []\n",
    "    output_width, output_height = output_size\n",
    "    \n",
    "    with rasterio.open(f'Datasets/{img_path}') as src:\n",
    "        print(\"Available band indices:\", src.indexes)\n",
    "        print(\"Band descriptions:\", src.descriptions)   \n",
    "        \n",
    "        # Determine the band indices to process\n",
    "        if selected_bands == ['NDVI']:\n",
    "            bands_to_process = [1]  # Assuming NDVI is stored in the first band\n",
    "        else:\n",
    "            bands_to_process = selected_bands\n",
    "        \n",
    "        for band_idx in bands_to_process:\n",
    "            # Read the band\n",
    "            band_data = src.read(band_idx)\n",
    "\n",
    "            # Normalize and clip NDVI if applicable\n",
    "            if selected_bands == ['NDVI']:\n",
    "                band_data = np.clip(band_data, -1, 1)  # Clip to valid NDVI range\n",
    "                band_data = (band_data + 1) / 2  # Normalize to [0, 1]\n",
    "            else:\n",
    "                # Normalize to [0, 1] based on actual range\n",
    "                band_min, band_max = np.nanmin(band_data), np.nanmax(band_data)\n",
    "                band_data = (band_data - band_min) / (band_max - band_min + 1e-10)\n",
    "            \n",
    "            # Replace NaN values with 0\n",
    "            band_data = np.nan_to_num(band_data, nan=0.0)\n",
    "\n",
    "            # Resize the band using OpenCV\n",
    "            band_resized = cv2.resize(band_data, (output_width, output_height), interpolation=cv2.INTER_AREA)\n",
    "            resized_bands.append(band_resized.astype(np.float32))\n",
    "    \n",
    "    # Stack bands into a single multi-channel image (height, width, channels)\n",
    "    processed_image = np.stack(resized_bands, axis=-1)\n",
    "    \n",
    "    print(f\"Processed image shape: {processed_image.shape}\")  # Debugging\n",
    "    return processed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a704ae8-69dd-451f-9381-1d4235e75043",
   "metadata": {},
   "source": [
    "#### Export images with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ff720-8abf-489b-9289-848fcc6763c7",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network (CNN) model for classification  \n",
    "https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfe774ca-5990-4354-bee0-041dc2611e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_data_csv = pd.read_csv('image_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6efb052d-1b32-4ffd-90e2-3d5bf510eb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5157376 , 0.5081389 , 0.50550914, ..., 0.5893659 , 0.59007025,\n",
       "        0.58577627],\n",
       "       [0.52456516, 0.51278675, 0.5180901 , ..., 0.5771186 , 0.5890899 ,\n",
       "        0.58999   ],\n",
       "       [0.4697992 , 0.5124007 , 0.51131797, ..., 0.56913704, 0.57615364,\n",
       "        0.58365315],\n",
       "       ...,\n",
       "       [0.50820225, 0.51105773, 0.5123773 , ..., 0.5650953 , 0.56014436,\n",
       "        0.5519329 ],\n",
       "       [0.5091227 , 0.51121265, 0.5116143 , ..., 0.56189483, 0.56021184,\n",
       "        0.55427855],\n",
       "       [0.5086429 , 0.5098837 , 0.511041  , ..., 0.55685735, 0.5673195 ,\n",
       "        0.5656968 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndvi_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "197e4490-3574-4b6a-b10e-d33dc4a49119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.4614 - loss: 0.6895 - val_accuracy: 0.3889 - val_loss: 1.4579\n",
      "Epoch 2/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 657ms/step - accuracy: 0.5264 - loss: 1.0071 - val_accuracy: 0.3889 - val_loss: 0.6943\n",
      "Epoch 3/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 612ms/step - accuracy: 0.4835 - loss: 0.6939 - val_accuracy: 0.3889 - val_loss: 0.6932\n",
      "Epoch 4/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 580ms/step - accuracy: 0.5499 - loss: 0.6931 - val_accuracy: 0.3889 - val_loss: 0.6934\n",
      "Epoch 5/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 545ms/step - accuracy: 0.5303 - loss: 0.6931 - val_accuracy: 0.3889 - val_loss: 0.6937\n",
      "Epoch 6/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 546ms/step - accuracy: 0.5655 - loss: 0.6929 - val_accuracy: 0.3889 - val_loss: 0.6939\n",
      "Epoch 7/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 559ms/step - accuracy: 0.5225 - loss: 0.6930 - val_accuracy: 0.3889 - val_loss: 0.6940\n",
      "Epoch 8/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 553ms/step - accuracy: 0.5225 - loss: 0.6930 - val_accuracy: 0.3889 - val_loss: 0.6943\n",
      "Epoch 9/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 571ms/step - accuracy: 0.5577 - loss: 0.6926 - val_accuracy: 0.3889 - val_loss: 0.6946\n",
      "Epoch 10/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 571ms/step - accuracy: 0.5264 - loss: 0.6928 - val_accuracy: 0.3889 - val_loss: 0.6947\n",
      "Epoch 11/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 567ms/step - accuracy: 0.5147 - loss: 0.6929 - val_accuracy: 0.3889 - val_loss: 0.6949\n",
      "Epoch 12/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 610ms/step - accuracy: 0.5538 - loss: 0.6923 - val_accuracy: 0.3889 - val_loss: 0.6952\n",
      "Epoch 13/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 536ms/step - accuracy: 0.5420 - loss: 0.6924 - val_accuracy: 0.3889 - val_loss: 0.6954\n",
      "Epoch 14/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 567ms/step - accuracy: 0.5303 - loss: 0.6926 - val_accuracy: 0.3889 - val_loss: 0.6954\n",
      "Epoch 15/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 558ms/step - accuracy: 0.5499 - loss: 0.6922 - val_accuracy: 0.3889 - val_loss: 0.6954\n",
      "Epoch 16/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 573ms/step - accuracy: 0.5303 - loss: 0.6926 - val_accuracy: 0.3889 - val_loss: 0.6953\n",
      "Epoch 17/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 561ms/step - accuracy: 0.5108 - loss: 0.6930 - val_accuracy: 0.3889 - val_loss: 0.6953\n",
      "Epoch 18/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 558ms/step - accuracy: 0.5342 - loss: 0.6925 - val_accuracy: 0.3889 - val_loss: 0.6956\n",
      "Epoch 19/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 581ms/step - accuracy: 0.5577 - loss: 0.6920 - val_accuracy: 0.3889 - val_loss: 0.6959\n",
      "Epoch 20/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 613ms/step - accuracy: 0.5460 - loss: 0.6921 - val_accuracy: 0.3889 - val_loss: 0.6960\n",
      "Epoch 21/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 556ms/step - accuracy: 0.5186 - loss: 0.6928 - val_accuracy: 0.3889 - val_loss: 0.6962\n",
      "Epoch 22/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 568ms/step - accuracy: 0.5499 - loss: 0.6919 - val_accuracy: 0.3889 - val_loss: 0.6964\n",
      "Epoch 23/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 578ms/step - accuracy: 0.5225 - loss: 0.6926 - val_accuracy: 0.3889 - val_loss: 0.6966\n",
      "Epoch 24/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 542ms/step - accuracy: 0.5225 - loss: 0.6926 - val_accuracy: 0.3889 - val_loss: 0.6970\n",
      "Epoch 25/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 587ms/step - accuracy: 0.5264 - loss: 0.6924 - val_accuracy: 0.3889 - val_loss: 0.6972\n",
      "Epoch 26/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 569ms/step - accuracy: 0.5108 - loss: 0.6929 - val_accuracy: 0.3889 - val_loss: 0.6974\n",
      "Epoch 27/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 571ms/step - accuracy: 0.4913 - loss: 0.6936 - val_accuracy: 0.3889 - val_loss: 0.6976\n",
      "Epoch 28/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 600ms/step - accuracy: 0.5420 - loss: 0.6917 - val_accuracy: 0.3889 - val_loss: 0.6978\n",
      "Epoch 29/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 578ms/step - accuracy: 0.5460 - loss: 0.6915 - val_accuracy: 0.3889 - val_loss: 0.6977\n",
      "Epoch 30/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 563ms/step - accuracy: 0.4991 - loss: 0.6934 - val_accuracy: 0.3889 - val_loss: 0.6974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15375478fd0>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model for binary classification\n",
    "height = 256\n",
    "width = 256\n",
    "num_channels = 3\n",
    "num_channels_input = num_channels  # Depends on the image bands\n",
    "num_classes = 1  # Binary classification (output either 0 or 1)\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(height, width, num_channels_input), padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='sigmoid')  # Sigmoid for binary classification\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_images_from_directory(image_dir, target_height=256, target_width=256, num_channels=3):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(\".tif\") or filename.endswith(\".png\"):  # Adjust based on your file types\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)  # Read image (adjust as needed)\n",
    "            img = cv2.resize(img, (target_width, target_height))  # Resize image\n",
    "            if img.ndim == 2:  # If grayscale, repeat channels\n",
    "                img = np.stack([img] * num_channels, axis=-1)\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load and preprocess images\n",
    "image_dir = 'Datasets/Model_Ready/'  # Replace with the actual path\n",
    "X = load_images_from_directory(image_dir, target_height=256, target_width=256, num_channels=3)\n",
    "\n",
    "# Normalize images\n",
    "X_resized = X.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Example labels (replace with actual labels for your dataset)\n",
    "y_expanded = np.concatenate([np.repeat(0, X_resized.shape[0] // 2), np.repeat(1, X_resized.shape[0] // 2)])\n",
    "\n",
    "# Split data into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resized, y_expanded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compile [configure] the model with binary_crossentropy loss\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Now, you can train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1cb909-0c40-44b6-8a9f-36318ad5bf5b",
   "metadata": {},
   "source": [
    "First trained results:\n",
    "\n",
    "Epoch 1/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 56s 1s/step - accuracy: 0.4699 - loss: 1.0823 - val_accuracy: 0.4973 - val_loss: 0..693  \n",
    "Epoch 2/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.4916 - loss: 0.6934 - val_accuracy: 0.4973 - val_loss: 0. 0.6  \n",
    "Epoch 3/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.4791 - loss: 0.6932 - val_accuracy: 0.4973 - val_loss: 0.6931  \n",
    "Epoch 4/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 82s 1s/step - accuracy: 0.4822 - loss: 0.6932 - val_accuracy: 0.4973 - val_loss: 0.6932  \n",
    "Epoch 5/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.4857 - loss: 0.6932 - val_accuracy: 0.4973 - val_loss: 0.6932  \n",
    "Epoch 6/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.5116 - loss: 0.6931 - val_accuracy: 0.4973 - val_loss: 0.6932  \n",
    "Epoch 7/32  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.4958 - loss: 0.6932 - val_accuracy: 0.4973 - val_loss: 0.6932  \n",
    "Epoch 8/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.4910 - loss: 0.6932 - val_accuracy: 0.4973 - val_loss: 0.6932  \n",
    "Epoch 9/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.5157 - loss: 0.6931 - val_accuracy: 0.4973 - val_loss: 0.6932  \n",
    "Epoch 10/10  \n",
    "47/47 ━━━━━━━━━━━━━━━━━━━━ 51s 1s/step - accuracy: 0.4977 - loss: 0.6932 - val_accuracy: 0.4973 - val_loss: 0.6932  \n",
    "<keras.src.callbacks.history.History at 0x29e1db63cd0>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9cd3b-7463-491b-ad49-ee35fda5cf08",
   "metadata": {},
   "source": [
    " at 0x29e1db63cd0>\n",
    "\n",
    "Both training accuracy and validation accuracy hover around 49-51% and do not improve significantly over the epochs\n",
    "\n",
    "Both training and validation loss remain around 0.693 throughout the training. This is the typical loss value when using the binary_crossentropy loss function with random guessing.\n",
    "\n",
    "A loss of 0.693 is equivalent to the log loss for random predictions (since -log(0.5) = 0.693), meaning the model is not learning meaningful patrns from the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1823f-f6c6-4bb6-8a28-51674cc78ae2",
   "metadata": {},
   "source": [
    "Steps to improve on the above:\n",
    "\n",
    "Data Exploration:\n",
    "\n",
    "Explore the distribution of data, including NDVI values and labels. Make sure there’s a balance between the classes and that NDVI values are truly representative of the classes.\n",
    "\n",
    "Model Adjustments:\n",
    "\n",
    "Add more layers or experimenting with different layer configurations (e.g., using more filters or deeper convolutional layers).\n",
    "Also, experiment with dropout layers to prevent overfitting, or regularization techniques like L2.\n",
    "\n",
    "Feature Engineering:\n",
    "\n",
    "Include additional features beyond NDVI (if possible), such as other spectral indices or geographical/contextual information.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "Adjust the learning rate of the optimizer (try decreasing it) or experiment with different optimizers (e.g., RMSprop or SGD).\n",
    "\n",
    "More Training Data:\n",
    "\n",
    "Export more data to ensure the model is seeing a representative variety of both classes. This is especially important if the model is stuck at a low performance threshold.\n",
    "\n",
    "Check Data Alignment:\n",
    "\n",
    "Ensure the images and labels are aligned correctly and that the preprocessing steps like resizing and normalization do not remove essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2dea7-85b4-4461-ad62-6d7e25342dc6",
   "metadata": {},
   "source": [
    "Second training run:\n",
    "\n",
    "Epoch 1/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 619s 1s/step - accuracy: 0.5079 - loss: 0.7330 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 2/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 609s 1s/step - accuracy: 0.5015 - loss: 0.6932 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 3/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 607s 1s/step - accuracy: 0.5012 - loss: 0.6931 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 4/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 623s 1s/step - accuracy: 0.4980 - loss: 0.6932 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 5/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 610s 1s/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932  \n",
    "Epoch 6/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 32132s 59s/step - accuracy: 0.4901 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932  \n",
    "Epoch 7/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 996s 2s/step - accuracy: 0.5089 - loss: 0.6931 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 8/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 40294s 74s/step - accuracy: 0.4942 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932  \n",
    "Epoch 9/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 611s 1s/step - accuracy: 0.5069 - loss: 0.6932 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 10/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 621s 1s/step - accuracy: 0.5029 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932  \n",
    "<keras.src.callbacks.history.History at 0x27a9dd66610>\n",
    "\n",
    "\n",
    "Increasing the data to 549 samples is not proving to be more accurate since the results appear around 50% and the model is still stagnating and not learning.\n",
    "\n",
    "Increasing the model complexity and layers as a next step.\n",
    "\n",
    "More Convolutional Layers: Each block includes a convolutional layer followed by batch normalization and max-pooling to extract more complex features  \n",
    "Batch Normalization: This layer normalizes the inputs of the next layer, which often leads to faster training and better performance  \n",
    "Increased Filters: The number of filters increases as we move deeper into the network to allow the model to capture more complex patterns  \n",
    "Dropout: A dropout layer with a 50% dropout rate is added after the fully connected layer to reduce overfitting by randomly setting half of the neurons to zero during training.  \n",
    "Activation and Loss Function: Since this is a binary classification task, the final layer uses sigmoid and binary_crossentropy as the loss function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e9bfbcb-5560-494a-8128-9b3caca689fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhech\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5427s\u001b[0m 10s/step - accuracy: 0.4940 - loss: 1.9221 - val_accuracy: 0.5003 - val_loss: 1.5989\n",
      "Epoch 2/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132174s\u001b[0m 241s/step - accuracy: 0.4962 - loss: 0.6936 - val_accuracy: 0.4967 - val_loss: 19.5083\n",
      "Epoch 3/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2846s\u001b[0m 5s/step - accuracy: 0.4944 - loss: 0.7043 - val_accuracy: 0.4997 - val_loss: 0.7133\n",
      "Epoch 4/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2753s\u001b[0m 5s/step - accuracy: 0.5005 - loss: 0.6942 - val_accuracy: 0.5033 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2721s\u001b[0m 5s/step - accuracy: 0.4926 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932\n",
      "Epoch 6/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2751s\u001b[0m 5s/step - accuracy: 0.4928 - loss: 0.6935 - val_accuracy: 0.4972 - val_loss: 0.6940\n",
      "Epoch 7/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2866s\u001b[0m 5s/step - accuracy: 0.4967 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932\n",
      "Epoch 8/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2833s\u001b[0m 5s/step - accuracy: 0.4969 - loss: 0.6936 - val_accuracy: 0.5033 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2741s\u001b[0m 5s/step - accuracy: 0.4981 - loss: 0.6933 - val_accuracy: 0.4983 - val_loss: 8.6245\n",
      "Epoch 10/10\n",
      "\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2914s\u001b[0m 5s/step - accuracy: 0.5031 - loss: 0.6934 - val_accuracy: 0.5033 - val_loss: 0.6931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27a96c2b150>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model for binary classification\n",
    "height = 256\n",
    "width = 256\n",
    "num_channels_input = num_channels  # Depends on the image bands\n",
    "num_classes = 1  # Binary classification (output either 0 or 1)\n",
    "\n",
    "model = models.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(height, width, num_channels_input)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Second convolutional block\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Third convolutional block\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Fourth convolutional block\n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Fifth convolutional block\n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Flatten the output of the convolutional layers\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Fully connected layer with dropout\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout to prevent overfitting\n",
    "\n",
    "    # Output layer with softmax for binary classification\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile [configure] the model with binary_crossentropy loss\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ensure your input data has the correct shape\n",
    "X_resized = np.resize(X, (X.shape[0], 256, 256, 1))  # Shape: (num_samples, 256, 256, 1)\n",
    "y_expanded = np.concatenate([np.repeat(0, X_resized.shape[0] // 2), np.repeat(1, X_resized.shape[0] // 2)])\n",
    "\n",
    "# Split data into training and validation sets (e.g., 80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resized, y_expanded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model (assuming X_train is the image data and y_train are labels)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d72a7-2d4e-41e4-af65-29935364e6d7",
   "metadata": {},
   "source": [
    "Epoch 1/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 5427s 10s/step - accuracy: 0.4940 - loss: 1.9221 - val_accuracy: 0.5003 - val_loss: 1.5989  \n",
    "Epoch 2/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 132174s 241s/step - accuracy: 0.4962 - loss: 0.6936 - val_accuracy: 0.4967 - val_loss: 19.5083  \n",
    "Epoch 3/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2846s 5s/step - accuracy: 0.4944 - loss: 0.7043 - val_accuracy: 0.4997 - val_loss: 0.7133  \n",
    "Epoch 4/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2753s 5s/step - accuracy: 0.5005 - loss: 0.6942 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 5/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2721s 5s/step - accuracy: 0.4926 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932  \n",
    "Epoch 6/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2751s 5s/step - accuracy: 0.4928 - loss: 0.6935 - val_accuracy: 0.4972 - val_loss: 0.6940  \n",
    "Epoch 7/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2866s 5s/step - accuracy: 0.4967 - loss: 0.6932 - val_accuracy: 0.4967 - val_loss: 0.6932  \n",
    "Epoch 8/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2833s 5s/step - accuracy: 0.4969 - loss: 0.6936 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "Epoch 9/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2741s 5s/step - accuracy: 0.4981 - loss: 0.6933 - val_accuracy: 0.4983 - val_loss: 8.6245  \n",
    "Epoch 10/10  \n",
    "549/549 ━━━━━━━━━━━━━━━━━━━━ 2914s 5s/step - accuracy: 0.5031 - loss: 0.6934 - val_accuracy: 0.5033 - val_loss: 0.6931  \n",
    "<keras.src.callbacks.history.History at 0x27a96c2b150>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a47e5721-5c30-4ddc-9058-98469705a2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbbfacce0f14b6fbd7a6f172499b759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[46.529, 6.746], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a year to be used [used for further processing]\n",
    "year = 2013\n",
    "dataset_image_land_07 = ee.Image('LANDSAT/LE07/C02/T1_TOA/LE07_195027_19990725').select(\n",
    "    ['B3', 'B2', 'B1']\n",
    ")\n",
    "\n",
    "dataset_land_07 = ee.ImageCollection('LANDSAT/LE07/C02/T1_TOA').filterDate(\n",
    "    '2001-01-01', '2002-12-31'\n",
    ").select(['B3', 'B2', 'B1'])\n",
    "trueColor321 = dataset_land_07.select(['B3', 'B2', 'B1'])\n",
    "trueColor321Vis = {\n",
    "    'min': 0.0,\n",
    "    'max': 0.4,\n",
    "    'gamma': 1.2,\n",
    "}\n",
    "m = geemap.Map()\n",
    "m.set_center(6.746, 46.529, 6)\n",
    "m.add_layer(trueColor321, trueColor321Vis, 'True Color (321)')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8ce9f-b02d-489f-9b6b-aaaf85a591d6",
   "metadata": {},
   "source": [
    "The selection of bands B4 (red) and B5 (near-infrared) for calculating NDVI is correct for Landsat data. These are the standard bands used to assess vegetation health, as they emphasize the contrast between vegetation and other land cover types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "36657947-675c-4b7b-955f-168b871453b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafc15ed68e64a84b03db9964939e9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[46.529, 6.746], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_image_land_08 = ee.Image('LANDSAT/LC08/C02/T1_TOA/LC08_199027_20170831').select(\n",
    "    ['B3', 'B2', 'B1']\n",
    ")\n",
    "\n",
    "dataset_land_08 = ee.ImageCollection('LANDSAT/LC08/C02/T1_TOA').filterDate(\n",
    "    f'{year}-01-01', f'{year}-12-31'\n",
    ").select(['B4', 'B5', 'B2'])\n",
    "true_color_432 = dataset_land_08.select(['B4', 'B5'])\n",
    "true_color_432_vis = {\n",
    "    'min': 0.0,\n",
    "    'max': 0.4,\n",
    "}\n",
    "m = geemap.Map()\n",
    "m.set_center(6.746, 46.529, 6)\n",
    "m.add_layer(true_color_432, true_color_432_vis, 'True Color (432)')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e3bb0-d10a-439e-a024-6b812387d8ae",
   "metadata": {},
   "source": [
    "Longitude: 7.171219710217076  \r\n",
    "​Latitude: 47.24405762918135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0a9675b9-00ad-415e-b1e3-177ab021fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - first line is standard approach for encompassing the region\n",
    "# geometry = ee.Geometry.Rectangle([-8, 37, 33,52])\n",
    "\n",
    "# - second line of code is with above map points and a buffer\n",
    "point = ee.Geometry.Point([40.28, 61.28])\n",
    "\n",
    "# Create a buffer of 5 kilometers\n",
    "buffer = point.buffer(5000)\n",
    "\n",
    "# Use the buffer or the point as the region for export\n",
    "geometry = buffer.bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a1cfa6-0701-4d04-abbf-49e8a1aacd4c",
   "metadata": {},
   "source": [
    "Projections:\n",
    "https://developers.google.com/earth-engine/guides/projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d40dbe69-ef26-418d-b3ff-ea37fe3cefc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>:root {\n",
       "  --font-color-primary: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --font-color-secondary: var(--jp-content-font-color2, rgba(0, 0, 0, 0.6));\n",
       "  --font-color-accent: rgba(123, 31, 162, 1);\n",
       "  --border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --background-color: var(--jp-layout-color0, white);\n",
       "  --background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=\"dark\"],\n",
       "body[data-theme=\"dark\"],\n",
       "body.vscode-dark {\n",
       "  --font-color-primary: rgba(255, 255, 255, 1);\n",
       "  --font-color-secondary: rgba(255, 255, 255, 0.6);\n",
       "  --font-color-accent: rgb(173, 132, 190);\n",
       "  --border-color: #2e2e2e;\n",
       "  --background-color: #111111;\n",
       "  --background-color-row-even: #111111;\n",
       "  --background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".ee {\n",
       "  padding: 1em;\n",
       "  line-height: 1.5em;\n",
       "  min-width: 300px;\n",
       "  max-width: 1200px;\n",
       "  overflow-y: scroll;\n",
       "  max-height: 600px;\n",
       "  border: 1px solid var(--border-color);\n",
       "  font-family: monospace;\n",
       "}\n",
       "\n",
       ".ee li {\n",
       "  list-style-type: none;\n",
       "}\n",
       "\n",
       ".ee ul {\n",
       "  padding-left: 1.5em !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".ee > ul {\n",
       "  padding-left: 0 !important;\n",
       "}\n",
       "\n",
       ".ee-open,\n",
       ".ee-shut {\n",
       "  color: var(--font-color-secondary);\n",
       "  cursor: pointer;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".ee-open:hover,\n",
       ".ee-shut:hover {\n",
       "  color: var(--font-color-primary);\n",
       "}\n",
       "\n",
       ".ee-k {\n",
       "  color: var(--font-color-accent);\n",
       "  margin-right: 6px;\n",
       "}\n",
       "\n",
       ".ee-v {\n",
       "  color: var(--font-color-primary);\n",
       "}\n",
       "\n",
       ".ee-toggle {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".ee-shut + ul {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".ee-open + ul {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".ee-shut::before {\n",
       "  display: inline-block;\n",
       "  content: \"▼\";\n",
       "  margin-right: 6px;\n",
       "  transform: rotate(-90deg);\n",
       "  transition: transform 0.2s;\n",
       "}\n",
       "\n",
       ".ee-open::before {\n",
       "  transform: rotate(0deg);\n",
       "  display: inline-block;\n",
       "  content: \"▼\";\n",
       "  margin-right: 6px;\n",
       "  transition: transform 0.2s;\n",
       "}\n",
       "</style><div class='ee'><ul><li><span class='ee-v'>106830</span></li></ul></div><script>function toggleHeader() {\n",
       "    const parent = this.parentElement;\n",
       "    parent.className = parent.className === \"ee-open\" ? \"ee-shut\" : \"ee-open\";\n",
       "}\n",
       "\n",
       "for (let c of document.getElementsByClassName(\"ee-toggle\")) {\n",
       "    c.onclick = toggleHeader;\n",
       "}</script></div>"
      ],
      "text/plain": [
       "<ee.ee_number.Number at 0x15379d19c10>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_land_08.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aba3d5f2-72e4-41e9-9c43-3e2174d57cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_landsat_with_ndvi = dataset_land_08.limit(20).map(compute_ndvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4edb3a3-9a1f-41df-9474-1db1d08d8304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a median composite of the NDVI band\n",
    "ndvi_landsat_08_composite = dataset_landsat_with_ndvi.select('NDVI').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e06af04-1285-48e0-b40b-6afa9ce22f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>:root {\n",
       "  --font-color-primary: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --font-color-secondary: var(--jp-content-font-color2, rgba(0, 0, 0, 0.6));\n",
       "  --font-color-accent: rgba(123, 31, 162, 1);\n",
       "  --border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --background-color: var(--jp-layout-color0, white);\n",
       "  --background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=\"dark\"],\n",
       "body[data-theme=\"dark\"],\n",
       "body.vscode-dark {\n",
       "  --font-color-primary: rgba(255, 255, 255, 1);\n",
       "  --font-color-secondary: rgba(255, 255, 255, 0.6);\n",
       "  --font-color-accent: rgb(173, 132, 190);\n",
       "  --border-color: #2e2e2e;\n",
       "  --background-color: #111111;\n",
       "  --background-color-row-even: #111111;\n",
       "  --background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".ee {\n",
       "  padding: 1em;\n",
       "  line-height: 1.5em;\n",
       "  min-width: 300px;\n",
       "  max-width: 1200px;\n",
       "  overflow-y: scroll;\n",
       "  max-height: 600px;\n",
       "  border: 1px solid var(--border-color);\n",
       "  font-family: monospace;\n",
       "}\n",
       "\n",
       ".ee li {\n",
       "  list-style-type: none;\n",
       "}\n",
       "\n",
       ".ee ul {\n",
       "  padding-left: 1.5em !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".ee > ul {\n",
       "  padding-left: 0 !important;\n",
       "}\n",
       "\n",
       ".ee-open,\n",
       ".ee-shut {\n",
       "  color: var(--font-color-secondary);\n",
       "  cursor: pointer;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".ee-open:hover,\n",
       ".ee-shut:hover {\n",
       "  color: var(--font-color-primary);\n",
       "}\n",
       "\n",
       ".ee-k {\n",
       "  color: var(--font-color-accent);\n",
       "  margin-right: 6px;\n",
       "}\n",
       "\n",
       ".ee-v {\n",
       "  color: var(--font-color-primary);\n",
       "}\n",
       "\n",
       ".ee-toggle {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".ee-shut + ul {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".ee-open + ul {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".ee-shut::before {\n",
       "  display: inline-block;\n",
       "  content: \"▼\";\n",
       "  margin-right: 6px;\n",
       "  transform: rotate(-90deg);\n",
       "  transition: transform 0.2s;\n",
       "}\n",
       "\n",
       ".ee-open::before {\n",
       "  transform: rotate(0deg);\n",
       "  display: inline-block;\n",
       "  content: \"▼\";\n",
       "  margin-right: 6px;\n",
       "  transition: transform 0.2s;\n",
       "}\n",
       "</style><div class='ee'><ul><li><label class='ee-shut'>Image (1 band)<input type='checkbox' class='ee-toggle'></label><ul><li><span class='ee-k'>type:</span><span class='ee-v'>Image</span></li><li><label class='ee-shut'>bands: List (1 element)<input type='checkbox' class='ee-toggle'></label><ul><li><label class='ee-shut'>0: \"NDVI\", float, EPSG:4326<input type='checkbox' class='ee-toggle'></label><ul><li><span class='ee-k'>id:</span><span class='ee-v'>NDVI</span></li><li><span class='ee-k'>crs:</span><span class='ee-v'>EPSG:4326</span></li><li><label class='ee-shut'>crs_transform: [1, 0, 0, 0, 1, 0]<input type='checkbox' class='ee-toggle'></label><ul><li><span class='ee-k'>0:</span><span class='ee-v'>1</span></li><li><span class='ee-k'>1:</span><span class='ee-v'>0</span></li><li><span class='ee-k'>2:</span><span class='ee-v'>0</span></li><li><span class='ee-k'>3:</span><span class='ee-v'>0</span></li><li><span class='ee-k'>4:</span><span class='ee-v'>1</span></li><li><span class='ee-k'>5:</span><span class='ee-v'>0</span></li></ul></li><li><label class='ee-shut'>data_type: float<input type='checkbox' class='ee-toggle'></label><ul><li><span class='ee-k'>type:</span><span class='ee-v'>PixelType</span></li><li><span class='ee-k'>max:</span><span class='ee-v'>1</span></li><li><span class='ee-k'>min:</span><span class='ee-v'>-1</span></li><li><span class='ee-k'>precision:</span><span class='ee-v'>float</span></li></ul></li></ul></li></ul></li></ul></li></ul></div><script>function toggleHeader() {\n",
       "    const parent = this.parentElement;\n",
       "    parent.className = parent.className === \"ee-open\" ? \"ee-shut\" : \"ee-open\";\n",
       "}\n",
       "\n",
       "for (let c of document.getElementsByClassName(\"ee-toggle\")) {\n",
       "    c.onclick = toggleHeader;\n",
       "}</script></div>"
      ],
      "text/plain": [
       "<ee.image.Image at 0x270324f5590>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndvi_landsat_08_composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b10f9c4d-7aaa-418e-a9b3-42b500ea1024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "export(ndvi_landsat_08_composite, geometry, 'Landsat_08_NDVI_Export_06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "545498b4-b60a-41fc-b18d-fcd3884209a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export the image, specifying the CRS, transform, and region.\n",
    "def export_task (dataset, geometry, description):\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=dataset,\n",
    "        description=description,\n",
    "        # crs=projection.get('crs'),\n",
    "        # crsTransform=projection['transform'],\n",
    "        scale=30, # 30 for Landsat\n",
    "        region=geometry,\n",
    "        maxPixels=1e13  # Increase max pixels to allow large exports\n",
    "    )\n",
    "    task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7da73-6c97-444b-a8da-44b21d66dc78",
   "metadata": {},
   "source": [
    "EEException: User memory limit exceeded. > from entire dataset. Export in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "65aabeda-6739-4c7a-9b87-88474a685620",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function to export each batch\n",
    "def export_monthly_batch(collection, start_date, end_date, batch_id):\n",
    "    # Filter collection by date\n",
    "    monthly_collection = collection.filterDate(start_date, end_date) \n",
    "    \n",
    "    # Create a composite image for the batch\n",
    "    composite_image = monthly_collection.median()\n",
    "\n",
    "    task = export_task(composite_image, geometry, 'Landsat_08_NDVI_Export_13')\n",
    "\n",
    "    if debug_enabled:\n",
    "        print(f\"Exporting batch {batch_id} from {start_date} to {end_date}\")\n",
    "\n",
    "# Define the list of monthly batches\n",
    "dates = [\n",
    "    (f'{year}-01-01', f'{year}-01-31'),\n",
    "    (f'{year}-02-01', f'{year}-02-28'),\n",
    "    (f'{year}-03-01', f'{year}-03-31'),\n",
    "    (f'{year}-04-01', f'{year}-04-30'),\n",
    "    (f'{year}-05-01', f'{year}-05-31'),\n",
    "    (f'{year}-06-01', f'{year}-06-30'),\n",
    "    (f'{year}-07-01', f'{year}-07-31'),\n",
    "    (f'{year}-08-01', f'{year}-08-31'),\n",
    "    (f'{year}-09-01', f'{year}-09-30'),\n",
    "    (f'{year}-10-01', f'{year}-10-31'),\n",
    "    (f'{year}-11-01', f'{year}-11-30'),\n",
    "    (f'{year}-12-01', f'{year}-12-31')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "144827e9-4fac-4bb3-b768-c7f4a6b4b107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop over each date range and export each month as a separate batch\n",
    "for i, (start_date, end_date) in enumerate(dates):\n",
    "    export_monthly_batch(dataset_land_08, start_date, end_date, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3beab92b-f3a7-4748-ae4e-b8dcd48b0b67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(1).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(2).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(3).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(4).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(5).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(6).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(7).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(8).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13(9).tif.\n",
      "Saved resized image with 3 band(s) to Datasets/Model_Ready/Landsat_08_NDVI_Export_13.tif.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhech\\anaconda3\\Lib\\site-packages\\skimage\\transform\\_warps.py:738: RuntimeWarning: All-NaN slice encountered\n",
      "  min_val = min_func(input_image)\n",
      "C:\\Users\\zhech\\anaconda3\\Lib\\site-packages\\skimage\\transform\\_warps.py:742: RuntimeWarning: All-NaN slice encountered\n",
      "  max_val = max_func(input_image)\n"
     ]
    }
   ],
   "source": [
    "# Images to be formatted folder name\n",
    "folder_name = 'Resize_function'\n",
    "# loop through all downloaded images and save for model training\n",
    "directory_data = os.fsencode(r\"C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Landsat\\{}\".format(folder_name))\n",
    "\n",
    "for file in os.listdir(directory_data):\n",
    "    filename = os.fsdecode(file)\n",
    "    load_geotiff_img_advanced(\"Landsat/Resize_function/{}\".format(filename), \"Datasets/Model_Ready/{}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb99907-f5dd-4f55-b4a7-049fbd45bab5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# *legacy function\n",
    "\n",
    "# Define Image Loading and Preprocessing Functions\n",
    "def load_geotiff(filepath):\n",
    "    with rasterio.open(filepath) as src:\n",
    "        # Read all bands and return as numpy array\n",
    "        image = src.read()  # Shape: (bands, height, width)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Check the shape of the image\n",
    "    if image.ndim == 3 and image.shape[0] == 1:  # Shape is (1, height, width)\n",
    "        image = image[0]  # Remove the first dimension\n",
    "\n",
    "    # Now check the shape again\n",
    "    if len(image.shape) == 2:  # Grayscale image with shape (height, width)\n",
    "        # Convert grayscale to 3 channels by stacking\n",
    "        image = np.stack((image,) * 3, axis=-1)  # Shape becomes (height, width, 3)\n",
    "    elif image.ndim == 3 and image.shape[2] == 3:  # RGB image\n",
    "        pass  # Already in the correct format\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected image shape: {}\".format(image.shape))\n",
    "\n",
    "    return image.astype(np.float32) / 255.0  # Normalize to [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e27be0ae-1874-43bc-9fad-ebffacf4ffd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_1.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_2.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_3.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_4.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000000000-0000000000.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000000000-0000032768.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000000000-0000065536.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000000000-0000098304.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000000000-0000131072.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000032768-0000000000.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000032768-0000032768.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000032768-0000065536.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000032768-0000098304.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_05-0000032768-0000131072.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(1).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(1).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(10).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(10).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(11).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(11).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(2).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(2).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(3).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(3).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(4).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(4).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(5).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(5).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(6).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(6).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(7).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(7).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(8).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(8).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(9).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07(9).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07.tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_07.tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(1).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(1).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(10).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(10).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(11).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(11).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(2).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(2).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(3).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(3).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(4).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(4).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(5).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(5).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(6).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(6).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(7).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(7).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(8).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(8).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(9).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08(9).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08.tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_08.tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(1).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(1).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(10).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(10).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(11).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(11).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(2).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(2).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(3).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(3).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(4).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(4).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(5).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(5).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(6).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(6).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(7).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(7).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(8).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(8).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(9).tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09(9).tif: Unexpected image shape: (3, 256, 256)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09.tif\n",
      "Error processing C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_09.tif: Unexpected image shape: (3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# *legacy function\n",
    "\n",
    "# Directory containing GeoTIFF images\n",
    "directory_model_data = r\"C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\"\n",
    "\n",
    "for file in os.listdir(directory_model_data):\n",
    "    # Construct the full file path\n",
    "    filename = os.path.join(directory_model_data, file)\n",
    "\n",
    "    # Print the filename to debug\n",
    "    if debug_enabled:\n",
    "        print(\"Loading file:\", filename)\n",
    "\n",
    "    # Load and preprocess each image\n",
    "    try:\n",
    "        # Check if it's a file, not a directory\n",
    "        if os.path.isfile(filename):\n",
    "            image = load_geotiff(filename)\n",
    "            processed_image = preprocess_image(image)\n",
    "            if debug_enabled:\n",
    "                print(\"Processed image shape:\", processed_image.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "45500dd0-2e0a-45df-a74a-f2de03aecb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_1.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_10.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_11.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_12.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_13.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_14.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_15.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_16.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_17.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_18.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_19.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_2.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_20.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_21.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_22.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_23.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_24.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_25.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_26.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_27.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_28.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_29.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_3.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_30.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_31.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_32.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_33.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_34.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_35.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_36.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_37.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_38.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_39.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_4.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_40.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_41.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_42.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_43.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_44.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_45.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_46.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_47.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_48.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_49.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_5.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_50.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_51.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_52.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_53.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_54.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_55.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_56.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_57.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_58.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_59.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_6.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_60.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_61.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_62.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_63.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_64.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_65.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_66.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_67.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_68.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_69.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_7.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_70.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_71.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_72.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_73.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_74.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_75.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_76.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_77.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_78.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_79.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_8.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_80.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_81.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_82.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_83.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_84.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_85.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_86.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Image_9.tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(1).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(2).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(3).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(4).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(5).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(6).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(7).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(8).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13(9).tif\n",
      "Processed image shape: (256, 256, 3)\n",
      "Loading file: C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\\Landsat_08_NDVI_Export_13.tif\n",
      "Processed image shape: (256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# *new functions\n",
    "\n",
    "# Define Image Loading and Preprocessing Functions\n",
    "def load_geotiff(filepath):\n",
    "    with rasterio.open(filepath) as src:\n",
    "        # Read all bands and return as numpy array\n",
    "        image = src.read()  # Shape: (bands, height, width)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Check the shape of the image\n",
    "    if image.ndim == 3:\n",
    "        if image.shape[0] == 1:  # Shape: (1, height, width)\n",
    "            # Single-channel grayscale image, remove the first dimension and expand to 3 channels\n",
    "            image = np.stack((image[0],) * 3, axis=-1)  # Shape becomes (height, width, 3)\n",
    "        elif image.shape[0] == 3 and image.shape[1:] != image.shape[0:1]:  # Shape: (3, height, width)\n",
    "            # Convert to (height, width, 3)\n",
    "            image = np.transpose(image, (1, 2, 0))  # Swap axes to (height, width, bands)\n",
    "        elif image.shape[2] == 3:  # Shape: (height, width, 3)\n",
    "            pass  # Already in the correct format\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected 3D image shape: {}\".format(image.shape))\n",
    "    elif image.ndim == 2:  # Grayscale image (height, width)\n",
    "        # Convert grayscale to 3 channels by stacking\n",
    "        image = np.stack((image,) * 3, axis=-1)  # Shape becomes (height, width, 3)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected image dimensions: {}\".format(image.ndim))\n",
    "\n",
    "    return image.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Directory containing GeoTIFF images\n",
    "directory_model_data = r\"C:\\Users\\zhech\\Desktop\\Softuni\\Machine Learning\\Final Project\\Datasets\\Model_Ready\"\n",
    "debug_enabled = True  # Enable debugging\n",
    "\n",
    "for file in os.listdir(directory_model_data):\n",
    "    # Construct the full file path\n",
    "    filename = os.path.join(directory_model_data, file)\n",
    "\n",
    "    # Print the filename to debug\n",
    "    if debug_enabled:\n",
    "        print(\"Loading file:\", filename)\n",
    "\n",
    "    # Load and preprocess each image\n",
    "    try:\n",
    "        # Check if it's a file, not a directory\n",
    "        if os.path.isfile(filename):\n",
    "            image = load_geotiff(filename)\n",
    "            processed_image = preprocess_image(image)\n",
    "            if debug_enabled:\n",
    "                print(\"Processed image shape:\", processed_image.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d2932-e630-44e3-809b-0337fb0d1a93",
   "metadata": {},
   "source": [
    "Create a Dataset Using TensorFlow’s tf.data API  \n",
    "- streamline loading, preprocessing, and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "564eb777-0fcc-4e8f-b755-0e9501f7d983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Image_1.tif': 1, 'Image_10.tif': 10, 'Image_11.tif': 11, 'Image_12.tif': 12, 'Image_13.tif': 13, 'Image_14.tif': 14, 'Image_15.tif': 15, 'Image_16.tif': 16, 'Image_17.tif': 17, 'Image_18.tif': 18, 'Image_19.tif': 19, 'Image_2.tif': 2, 'Image_20.tif': 20, 'Image_21.tif': 21, 'Image_22.tif': 22, 'Image_23.tif': 23, 'Image_24.tif': 24, 'Image_25.tif': 25, 'Image_26.tif': 26, 'Image_27.tif': 27, 'Image_28.tif': 28, 'Image_29.tif': 29, 'Image_3.tif': 3, 'Image_30.tif': 30, 'Image_31.tif': 31, 'Image_32.tif': 32, 'Image_33.tif': 33, 'Image_34.tif': 34, 'Image_35.tif': 35, 'Image_36.tif': 36, 'Image_37.tif': 37, 'Image_38.tif': 38, 'Image_39.tif': 39, 'Image_4.tif': 4, 'Image_40.tif': 40, 'Image_41.tif': 41, 'Image_42.tif': 42, 'Image_43.tif': 43, 'Image_44.tif': 44, 'Image_45.tif': 45, 'Image_46.tif': 46, 'Image_47.tif': 47, 'Image_48.tif': 48, 'Image_49.tif': 49, 'Image_5.tif': 5, 'Image_50.tif': 50, 'Image_51.tif': 51, 'Image_52.tif': 52, 'Image_53.tif': 53, 'Image_54.tif': 54, 'Image_55.tif': 55, 'Image_56.tif': 56, 'Image_57.tif': 57, 'Image_58.tif': 58, 'Image_59.tif': 59, 'Image_6.tif': 6, 'Image_60.tif': 60, 'Image_61.tif': 61, 'Image_62.tif': 62, 'Image_63.tif': 63, 'Image_64.tif': 64, 'Image_65.tif': 65, 'Image_66.tif': 66, 'Image_67.tif': 67, 'Image_68.tif': 68, 'Image_69.tif': 69, 'Image_7.tif': 7, 'Image_70.tif': 70, 'Image_71.tif': 71, 'Image_72.tif': 72, 'Image_73.tif': 73, 'Image_74.tif': 74, 'Image_75.tif': 75, 'Image_76.tif': 76, 'Image_77.tif': 77, 'Image_78.tif': 78, 'Image_79.tif': 79, 'Image_8.tif': 8, 'Image_80.tif': 80, 'Image_81.tif': 81, 'Image_82.tif': 82, 'Image_83.tif': 83, 'Image_84.tif': 84, 'Image_85.tif': 85, 'Image_86.tif': 86, 'Image_87.tif': 87, 'Image_88.tif': 88, 'Image_89.tif': 89, 'Image_9.tif': 9, 'Image_90.tif': 90, 'Image_91.tif': 91, 'Image_92.tif': 92, 'Image_93.tif': 93, 'Image_94.tif': 94, 'Image_95.tif': 95, 'Image_96.tif': 96}\n"
     ]
    }
   ],
   "source": [
    "labels_dict = {}\n",
    "for filename in os.listdir(directory_model_data):\n",
    "    if filename.endswith(\".tif\"):\n",
    "        # Example label extraction logic, modify according to your naming convention\n",
    "        label = int(filename.split(\"_\")[1].split(\".\")[0])  # Extract label from filename\n",
    "        labels_dict[filename] = label\n",
    "\n",
    "if debug_enabled:\n",
    "    print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "19dcbee8-ed58-46fa-b16b-d486a00f8a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(11.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(12.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(13.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(14.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(15.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(17.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(18.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(19.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(20.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(21.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(22.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(23.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(24.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(25.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(26.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(27.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(28.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(29.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(30.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(31.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(33.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(34.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(35.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(36.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(37.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(38.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(39.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(40.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(41.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(42.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(43.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(44.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(45.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(46.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(47.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(48.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(49.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(5.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(50.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(51.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(52.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(53.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(54.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(55.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(56.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(57.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(58.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(59.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(60.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(61.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(62.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(63.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(64.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(65.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(66.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(67.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(68.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(69.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(7.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(70.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(71.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(72.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(73.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(74.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(75.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(76.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(77.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(78.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(79.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(80.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(81.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(82.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(83.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(84.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(85.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(86.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(87.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(88.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(89.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(90.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(91.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(92.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(93.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(94.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(95.0, shape=(), dtype=float32)\n",
      "Image shape before yielding: (256, 256, 3)\n",
      "Yielding image shape: (256, 256, 3) and label: tf.Tensor(96.0, shape=(), dtype=float32)\n",
      "Image batch shape: (32, 256, 256, 3)\n",
      "Label batch shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "# Filepaths to the images\n",
    "filepaths_tif = glob.glob(directory_model_data + \"/*.tif\")\n",
    "# Total number of samples\n",
    "total_samples = len(filepaths_tif)\n",
    "\n",
    "# Generator function to load and preprocess each image\n",
    "def data_generator(filepaths, labels_dict):\n",
    "    for filepath in filepaths:\n",
    "        image = load_geotiff(filepath)  # Load the GeoTIFF image\n",
    "        image = preprocess_image(image)   # Preprocess the image\n",
    "\n",
    "        if debug_enabled:\n",
    "            print(f\"Image shape before yielding: {image.shape}\")  # Debugging line\n",
    "\n",
    "        # Convert the image to a TensorFlow tensor\n",
    "        image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "\n",
    "        # If image is (256, 256, 1), convert it to (256, 256, 3)\n",
    "        if image.shape[-1] == 1:\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "        # Extract filename and get the corresponding label\n",
    "        filename = os.path.basename(filepath)\n",
    "        label = labels_dict.get(filename)\n",
    "\n",
    "        # Skip if no label is found\n",
    "        if label is None:\n",
    "            continue\n",
    "        \n",
    "        # Convert label to a TensorFlow tensor\n",
    "        label = tf.convert_to_tensor(label, dtype=tf.float32)\n",
    "\n",
    "        if debug_enabled:\n",
    "            print(\"Yielding image shape:\", image.shape, \"and label:\", label)\n",
    "            \n",
    "        yield image, label\n",
    "\n",
    "# Create the dataset using a lambda function to pass labels_dict\n",
    "dataset_land_t = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(filepaths_tif, labels_dict),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),  # Image shape\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32)  # Label shape\n",
    "    )\n",
    ")\n",
    "\n",
    "# Shuffle, batch, and prefetch the dataset for efficient training\n",
    "dataset_land_t = dataset_land_t.shuffle(buffer_size=len(filepaths_tif)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Check dataset shapes\n",
    "for images, labels in dataset_land_t.take(1):\n",
    "    print(\"Image batch shape:\", images.shape)  # Should be (32, 256, 256, 3)\n",
    "    print(\"Label batch shape:\", labels.shape)  # Should be (32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "471e1649-24c5-4c3c-8c3d-5f5e4063aac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_34\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_34\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_148 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_149 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_150 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">67,109,120</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_148 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m1,792\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_52 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_149 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_53 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_150 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_54 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │      \u001b[38;5;34m67,109,120\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,512,961</span> (257.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,512,961\u001b[0m (257.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,512,961</span> (257.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,512,961\u001b[0m (257.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define input shape for Landsat imagery (e.g., 7 bands for Landsat 8)\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # First convolutional layer\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Second convolutional layer\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Third convolutional layer\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Flatten and add dense layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "\n",
    "    # Final layer - Adjust output units based on the task\n",
    "    tf.keras.layers.Dense(1, activation='linear')  # e.g., for regression tasks\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # Adjust loss function as per your needs\n",
    "\n",
    "# View the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0180e75-e218-44cb-8d0e-deb7ee633be2",
   "metadata": {},
   "source": [
    "Model summary shows a very high number of parameters in the fully connected layer dense_6, with over 67 million parameters. This occurs because the output from the last convolutional layer ((None, 32, 32, 256)) is flattened to (None, 262144) before being passed to dense_6, which has 256 units.\n",
    "\n",
    "To reduce the parameter count and potentially improve the model’s efficiency, consider adding another pooling layer or using a Global Average Pooling layer after the last convolutional layer. Global pooling layers reduce spatial dimensions without needing a high number of trainable parameters, which is helpful for satellite imagery tasks.\n",
    "\n",
    "Modifications\n",
    "Use an Additional Pooling Layer: You can add a MaxPooling2D layer after the third Conv2D layer to further down-sample the spatial dimensions.\n",
    "\n",
    "Use Global Average Pooling: Replace Flatten with GlobalAveragePooling2D to reduce the dimensionality more effectively. This will decrease the number of parameters by summarizing spatial dimensions.\n",
    "\n",
    "Global Average Pooling Layer: Reduces the spatial dimensions to a single value per channel (from (None, 32, 32, 256) to (None, 256)), which drastically reduces the input size to the fully connected layer.\n",
    "\n",
    "Impact on Parameters: This reduces the number of parameters in dense_6, making the model lighter and faster while retaining essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3fa405d3-8a0a-4b60-9e0a-512f28858b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_37\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_37\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ random_flip_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_rotation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_zoom_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomZoom</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_157 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_124              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_158 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_125              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_159 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_126              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ random_flip_10 (\u001b[38;5;33mRandomFlip\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_rotation_10 (\u001b[38;5;33mRandomRotation\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_zoom_10 (\u001b[38;5;33mRandomZoom\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_157 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m1,792\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_124              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_61 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_158 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_125              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_62 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_159 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_126              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_63 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_79 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_80 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_81 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">471,425</span> (1.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m471,425\u001b[0m (1.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">470,529</span> (1.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m470,529\u001b[0m (1.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    # Data Augmentation\n",
    "    tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    \n",
    "    # Convolutional layers\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'), # input_shape=input_shape\n",
    "    # BatchNormalization layers after convolutional layers to stabilize training and improve performance\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Global Average Pooling Layer\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    # Dense layers\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')  # For regression tasks\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "46af2559-0437-4e77-b2f5-6d59cb5edfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Split the Dataset into Training and Validation Sets \n",
    "# train_size = int(0.8 * len(filepaths_tif))\n",
    "# Calculate split sizes\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "\n",
    "train_dataset = dataset_land_t.take(train_size) #.repeat()\n",
    "val_dataset = dataset_land_t.skip(train_size) #.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3fb098e4-75bc-45fd-a9c3-e4db5b7a0975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define callbacks and monitor validation loss and prevent overfitting\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efcf8b-96c4-430c-9471-1598c022fac0",
   "metadata": {},
   "source": [
    "Optimizers: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "\n",
    "a. Regression (e.g., predicting NDVI, temperature, or other continuous variables)  \n",
    "Loss Function:  \n",
    "Mean Squared Error (MSE): Penalizes large errors more, suitable for most regression tasks.  \n",
    "Mean Absolute Error (MAE): Less sensitive to outliers, useful if you have noisy data.  \n",
    "Huber Loss: A combination of MSE and MAE, robust to outliers.  \n",
    "Optimizer:  \n",
    "Adam: A versatile and adaptive optimizer; often a good starting point.  \n",
    "RMSprop: Works well for noisy or non-stationary data.  \n",
    "SGD with Momentum: Can be effective but requires careful tuning.  \n",
    "b. Classification (e.g., land cover types, urban vs. rural)  \n",
    "Loss Function:  \n",
    "Categorical Cross-Entropy: For multi-class classification when labels are one-hot encoded.  \n",
    "Sparse Categorical Cross-Entropy: For multi-class classification with integer labels.  \n",
    "Binary Cross-Entropy: For binary classification tasks.  \n",
    "Optimizer:  \n",
    "Adam: Often provides a good balance of speed and accuracy.  \n",
    "SGD with Momentum: Good for fine-tuning pretrained models.  \n",
    "c. Segmentation (e.g., crop boundaries, water bodies, urban areas)  \n",
    "Loss Function:  \n",
    "Binary Cross-Entropy or Categorical Cross-Entropy: For pixel-wise classification.  \n",
    "Dice Loss: Focuses on the overlap between predicted and ground truth masks, often used for imbalanced datasets.  \n",
    "IoU (Jaccard) Loss: Useful when intersection-over-union is a key evaluation metric.  \n",
    "Optimizer:  \n",
    "Adam: A common choice for segmentation tasks.  \n",
    "SGD with Momentum: Can achieve better generalization on large datasets.  \n",
    "d. Object Detection (e.g., detecting buildings, roads)  \n",
    "Loss Function:  \n",
    "Custom Loss Functions: Typically combines classification loss (e.g., cross-entropy) and regression loss (e.g., smooth L1) for bounding boxes.  \n",
    "Optimizer:  \n",
    "AdamW: Adam with weight decay; suitable for complex architectures like YOLO or Faster R-CNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "82b879b8-c9c2-4546-80dc-3b34dc37fe00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "      3/Unknown \u001b[1m55s\u001b[0m 5s/step - loss: 3293.9270"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhech\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6s/step - loss: 3251.4226 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - loss: 3069.0876 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 3154.6499 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 3081.8047 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 2702.0378 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 2300.9773 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 1928.3229 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 1533.6788 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 1024.0300 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 777.4912 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 898.4009 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 735.1200 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 731.4362 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 694.7352 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 806.4094 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 835.3171 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 816.3223 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 847.4825 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 766.6549 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 762.1351 - learning_rate: 2.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 811.1532 - learning_rate: 2.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 861.8941 - learning_rate: 2.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 868.7731 - learning_rate: 2.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - loss: 842.0542 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # MSE for regression; adjust if needed\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001754af-abfd-4ac6-bcdd-af6b55ad1205",
   "metadata": {},
   "source": [
    "'2017-01-01', '2017-01-02'  \n",
    "'2022-01-01', '2022-01-02'\n",
    "\n",
    "\n",
    "First run:  \n",
    "Epoch 1/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step - loss: 1.2760  \n",
    "Epoch 2/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 183ms/step - loss: 1.2377  \n",
    "Epoch 3/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 176ms/step - loss: 1.4225  \n",
    "Epoch 4/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 182ms/step - loss: 0.9588  \n",
    "Epoch 5/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 181ms/step - loss: 1.0055  \n",
    "Epoch 6/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 269ms/step - loss: 1.9402  \n",
    "Epoch 7/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 198ms/step - loss: 1.0555  \n",
    "Epoch 8/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 197ms/step - loss: 1.3748  \n",
    "Epoch 9/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 236ms/step - loss: 1.4129  \n",
    "Epoch 10/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 291ms/step - loss: 1.2818  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c769ee3-52f7-404f-b4e0-efe05ceefbba",
   "metadata": {},
   "source": [
    "Second run:\n",
    "Results show a better overall trend. The training loss is consistently decreasing with each epoch, indicating that the model is gradually learning from the data. Starting from around 49.8 and decreasing to 15.3 suggests that the model is effectively minimizing the loss, which is typically a sign of convergence.\n",
    "\n",
    "Epoch 1/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step - loss: 49.7952  \n",
    "Epoch 2/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 535ms/step - loss: 46.4295  \n",
    "Epoch 3/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 495ms/step - loss: 43.4524  \n",
    "Epoch 4/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 525ms/step - loss: 39.2551  \n",
    "Epoch 5/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 646ms/step - loss: 35.0007  \n",
    "Epoch 6/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 559ms/step - loss: 31.6593  \n",
    "Epoch 7/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 542ms/step - loss: 27.3199  \n",
    "Epoch 8/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 522ms/step - loss: 22.7143  \n",
    "Epoch 9/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 586ms/step - loss: 18.0280  \n",
    "Epoch 10/10  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 536ms/step - loss: 15.3195  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19086a30-7121-4b91-a2a5-30983ce9f3e8",
   "metadata": {},
   "source": [
    "To further evaluate and improve:\n",
    "\n",
    "Increase Epochs: Running more epochs could help see if the loss continues to decrease or if it plateaus.\n",
    "Validation Loss Tracking: Adding validation data will help observe if the model is overfitting. A diverging validation loss with a decreasing training loss would indicate overfitting.\n",
    "Learning Rate: If the loss starts plateauing too soon, you could lower the learning rate further or try a learning rate scheduler for a gradual decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc552b6b-8368-4a9a-bbcc-7397e52789a3",
   "metadata": {},
   "source": [
    "Third run:  \n",
    "\n",
    "Epoch 1/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 3s 3s/step - loss: 22.7203  \n",
    "Epoch 2/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 20.6037  \n",
    "Epoch 3/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 14.4283  \n",
    "Epoch 4/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 18.2034  \n",
    "Epoch 5/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 18.6051  \n",
    "Epoch 6/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 19.6829  \n",
    "Epoch 7/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 16.8309  \n",
    "Epoch 8/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 16.3995  \n",
    "Epoch 9/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 16.7076  \n",
    "Epoch 10/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 18.6553  \n",
    "Epoch 11/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 16.3321  \n",
    "Epoch 12/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 17.2708  \n",
    "Epoch 13/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 18.6530  \n",
    "Epoch 14/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 17.1940  \n",
    "Epoch 15/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 16.5669  \n",
    "Epoch 16/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 16.7396  \n",
    "Epoch 17/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 16.1727  \n",
    "Epoch 18/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 14.5688  \n",
    "Epoch 19/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 12.9904  \n",
    "Epoch 20/20  \n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - loss: 18.8812  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c97136-0177-4638-8352-090472ff738d",
   "metadata": {},
   "source": [
    "Change bands to infrared B4, B5 for better training and use this dataset. Years here are 2022 and 2023 - two days are taken for both years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "00a12f92-2579-4918-8454-28560b959009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1296a031-5830-438f-b977-8cb5af0807dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [2495.2802734375,\n",
       "  2485.255126953125,\n",
       "  2422.899658203125,\n",
       "  2264.542236328125,\n",
       "  1967.07666015625,\n",
       "  1615.8690185546875,\n",
       "  1151.716552734375,\n",
       "  811.6048583984375,\n",
       "  643.9826049804688,\n",
       "  720.32470703125,\n",
       "  597.2861938476562,\n",
       "  625.9951782226562,\n",
       "  633.38134765625,\n",
       "  634.9278564453125,\n",
       "  618.8549194335938,\n",
       "  644.3345947265625,\n",
       "  658.4005737304688,\n",
       "  607.0859985351562,\n",
       "  632.1796264648438,\n",
       "  635.8745727539062],\n",
       " 'learning_rate': [0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513,\n",
       "  0.0010000000474974513]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f40f9-67dd-4ca1-a4c6-5e1965eef3a6",
   "metadata": {},
   "source": [
    "VGG16, ResNet50, IncetionV3, EfficientNet and DenseNet121 architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "af72cc68-3d8f-4aa6-b1e2-e8c572494c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66730f25-3a59-4b66-8bb6-4b758c7072e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False  # Freeze the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b53f18e6-dab1-4d66-baf2-4477c4e925b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e891b569-49c4-4c1c-affc-0f5042478db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "860dfffe-d938-4621-9bd0-058fc3ccb7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "81662059-9d91-4a06-8090-e4ef8206bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_27\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_27\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ densenet121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ densenet121 (\u001b[38;5;33mFunctional\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │       \u001b[38;5;34m7,037,504\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_56 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_57 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,168,833</span> (27.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,168,833\u001b[0m (27.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,329</span> (513.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m131,329\u001b[0m (513.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> (26.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,037,504\u001b[0m (26.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add custom layers\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='linear')  # Regression task\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ccddbaae-7dd3-48bc-9f66-0b47da2f4b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "     38/Unknown \u001b[1m129s\u001b[0m 2s/step - loss: 2510.7588"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m     train_dataset,\n\u001b[0;32m      3\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      6\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      7\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      8\u001b[0m     ]\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "15a41171-454b-4e6d-b2c3-977cc525d7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='--')\n",
    "plt.title('Model Loss Over Epochs [DenseNet121]', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2743404-c216-4f34-863c-892b9ce6459d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
